{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19d63de8",
   "metadata": {},
   "source": [
    "# 2 Preliminaries\n",
    "\n",
    "## 2.1 Data Manipulation\n",
    "\n",
    "### 2.1.1 Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6b9d2e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb861d4b",
   "metadata": {},
   "source": [
    "A tensor represents a array of numerical values. With one axis, a tensor is called a *vector*. With two axes, a tensor is called a *matrix*. With k > 2 axes, we drop the specialized names and just refer to the object as a $k^\\text{th}$ order tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "936fb0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arange(n) start at 0 included, end at n not included, default interval size is 1\n",
    "x = torch.arange(12, dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c4d77077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the total number of elements in a tensor\n",
    "x.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e2919cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access a tensor's shape\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7d3bc9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the shape of a tensor without altering its size or values\n",
    "X = x.reshape(3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fc36e49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also change the shape of a tensor if we don't know one of the size parameter.\n",
    "X1 = x.reshape(-1, 4)\n",
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c1a5495a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor with all zeros\n",
    "torch.zeros((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5f3faeb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor with all ones\n",
    "torch.ones((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9dd7938d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1114, -0.5883, -0.9171, -0.4005],\n",
       "        [ 0.0134, -1.1274,  0.8206, -0.5827],\n",
       "        [-1.0057,  1.2253,  0.1807,  1.4177]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor with elements drawn from a standard Gaussian (normal) distribution\n",
    "# with mean 0 and std 1\n",
    "torch.randn(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "81357b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 4, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [4, 3, 2, 1]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct tensors by supplying the exact values for each element\n",
    "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edf6e2de",
   "metadata": {},
   "source": [
    "### 2.1.2 Indexing and Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e977f7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]),\n",
       " tensor([[ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index as python lists\n",
    "X[-1], X[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4c748540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5., 17.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write elements of a matrix by specifying indices\n",
    "X[1, 2] = 17\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b2063398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 12., 12.],\n",
       "        [12., 12., 12., 12.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign multiple elements the same value\n",
    "X[:2, :] = 12\n",
    "X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f82d6f69",
   "metadata": {},
   "source": [
    "### 2.1.3 Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b52830f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([162754.7969, 162754.7969, 162754.7969, 162754.7969, 162754.7969,\n",
       "        162754.7969, 162754.7969, 162754.7969,   2980.9580,   8103.0840,\n",
       "         22026.4648,  59874.1406])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8767ef8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b93d086",
   "metadata": {},
   "source": [
    "We can also concatenate multiple tensors together, stacking them end-to end to form a larger tensor. We just need to provide a list of tensors and tell the system along which axis to concatenate. The example below shows what happens when we concatenate two matrices along rows (axis 0) vs. columns (axis 1). We can see that the first output's axis-0 length (6) is the sum of the two input tensors' axis-0 lengths $(3+3)$; while the second output's axis-1 length (8) is hte sum of the two input tensors' axis-1 lengths $(4+4)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "56eeb970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "192a792d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a binary tensor via logical statements.\n",
    "# if X[i, j] == Y[i, j], then the corresponding entry takes value 1, otherwise takes value 0\n",
    "X == Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "05fc1d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summing all elements in the tensor yields a tensor with only one element\n",
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01634049",
   "metadata": {},
   "source": [
    "### 2.1.4 Broadcasting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9321643",
   "metadata": {},
   "source": [
    "Under certain conditions, even when shapes differ, we can still perform elementwise binary operations by invoking the ***broadcasting*** mechanism.\n",
    "\n",
    "Broadcasting works according to the following two-step procedure:\n",
    "1. expand one or both arrays by copying elements along axes with length 1 so that after this transformation, the two tensors have the same shape\n",
    "2. perform an elementwise operation on the resulting arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "175692be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ecb3b097",
   "metadata": {},
   "source": [
    "Since a and b are $3 \\times 1$ and $1 \\times 2$ matrices, respectively, their shapes do not match up.\n",
    "\n",
    "Broadcasting produces a larger $3 \\times 2$ matrix by replicating matrix a along the columns and matrix b along the rows before adding them elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4d724033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d47c11",
   "metadata": {},
   "source": [
    "### 2.1.5 Saving Memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2166b845",
   "metadata": {},
   "source": [
    "Running operations can cause new memory to be allocated to host result. If write Y = X + Y, we dereference the tensor that Y used to point to and instead point Y at the newly allocated memory. We can demonstrate this issue with Python's id() function, which gives us the exact address of the referenced object in memory. Note that after we run Y = Y + X, id(Y) points to a different location. That's because Python first evaluates Y + X, allocating new memory for the result and then point Y to this new location in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "be2dbcdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "604ac45a",
   "metadata": {},
   "source": [
    "Undesirable for two reasons.\n",
    "1. We do not want to run around allocating memory unnecessarily all the time. We often have hundreds of megabytes of parameters and update all of them multiple times per second. We want to perform these updates in place.\n",
    "2. We might point at the same parameters from multiple variables. If we do not update in place, we must be careful to update all of these references, lest we spring a memory leak or inadvertently refer to stable parameters.\n",
    "\n",
    "Performing in-place operations is easy. We can assign the result of an operation to a previously allocated arry Y by using slice notation: `Y[:] = <expression>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ab2b65fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 5086955216\n",
      "id(Z): 5086955216\n"
     ]
    }
   ],
   "source": [
    "# overwrite the values of tensor Z, after initializing it, using zero_like, to have the same shape as Y\n",
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "125031ad",
   "metadata": {},
   "source": [
    "If the value of X is not reused in subsequent computations, we can also use `X[:] = X + Y or X += Y` to reduce the memory overhead of the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "853b852f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32587fb1",
   "metadata": {},
   "source": [
    "### 2.1.6 Conversion to Other Python Objects\n",
    "\n",
    "Converting a Numpy tensor (ndarray) is easy. The torch Tensor and numpy array will share their underlying memory, and changing one through an in-place operation will also change the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ec8ec875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = torch.from_numpy(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9e21e507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert a size-1 tensor to a python scalar, invoke item function\n",
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f4b01c1",
   "metadata": {},
   "source": [
    "### 2.1.7 Summary\n",
    "\n",
    "The tensor class is the main interface for storing and manipulating data in deep learning libraries. Tensors provide a variety of functions including construction routines; indexing and slicing; basic mathematics operations; broadcasting; memory-efficient assignment; and conversion to and from other Python objects."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba5862fe",
   "metadata": {},
   "source": [
    "### 2.1.8 Exercises\n",
    "\n",
    "1. Change the conditional statement `X == Y` to `X < Y` or `X > Y`, and then see what kind of tensor you can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b455b2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ True, False,  True, False],\n",
       "         [False, False, False, False],\n",
       "         [False, False, False, False]]),\n",
       " tensor([[False, False, False, False],\n",
       "         [ True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True]]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "X < Y, X > Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "132e0696",
   "metadata": {},
   "source": [
    "2. Replace the two tensors that operate by element in the broadcasting mechanism with other shapes, e.g., 3-dimensional tensors. Is the result the same as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ed502e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3514, -1.1736, -2.1658, -1.3078],\n",
       "         [ 0.7536, -0.7714, -1.7636, -0.9056],\n",
       "         [ 2.4033,  0.8783, -0.1139,  0.7441]],\n",
       "\n",
       "        [[ 0.8936, -0.6315, -1.6236, -0.7657],\n",
       "         [ 1.7422,  0.2172, -0.7750,  0.0830],\n",
       "         [ 0.2158, -1.3092, -2.3014, -1.4434]]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2, 3, 1)\n",
    "b = torch.randn(1, 1, 4)\n",
    "a, b\n",
    "a + b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5abfce87",
   "metadata": {},
   "source": [
    "## 2.2 Data Preprocessing\n",
    "### 2.2.1 Reading the Dataset\n",
    "\n",
    "To demonstrate how to load CSV file with pandas, create CSV file below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a180c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(os.path.join('..', 'data'), exist_ok=True)\n",
    "data_file = os.path.join('..', 'data', 'house_tiny.csv')\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('''NumRooms,RoofType,Price\n",
    "NA,NA,127500\n",
    "2,NA,106000\n",
    "4,Slate,178100\n",
    "NA,NA,140000''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c17ea270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms RoofType   Price\n",
      "0       NaN      NaN  127500\n",
      "1       2.0      NaN  106000\n",
      "2       4.0    Slate  178100\n",
      "3       NaN      NaN  140000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39d32f42",
   "metadata": {},
   "source": [
    "### 2.2.2 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "434dcf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  RoofType_Slate  RoofType_nan\n",
      "0       NaN               0             1\n",
      "1       2.0               0             1\n",
      "2       4.0               1             0\n",
      "3       NaN               0             1\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "07a990e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  RoofType_Slate  RoofType_nan\n",
      "0       3.0               0             1\n",
      "1       2.0               0             1\n",
      "2       4.0               1             0\n",
      "3       3.0               0             1\n"
     ]
    }
   ],
   "source": [
    "# replace the NaN entries with the mean value of the corresponding column\n",
    "inputs = inputs.fillna(inputs.mean())\n",
    "print(inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15bd66c2",
   "metadata": {},
   "source": [
    "### 2.2.3 Conversion to the Tensor Format\n",
    "\n",
    "All the entries in inputs and targets are numerical, we can load them into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1d5a9a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 0., 1.],\n",
       "         [2., 0., 1.],\n",
       "         [4., 1., 0.],\n",
       "         [3., 0., 1.]], dtype=torch.float64),\n",
       " tensor([127500, 106000, 178100, 140000]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X, y = torch.tensor(inputs.values), torch.tensor(targets.values)\n",
    "X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e60e5e6",
   "metadata": {},
   "source": [
    "### 2.2.4 Exercises\n",
    "\n",
    "1. Try loading datasets, e.g., Abalone from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.php) and inspect their properties. What fraction of them has missing values? What fraction of the variables is numerical, categorical, or text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "77f1e946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole_weight</th>\n",
       "      <th>shucked_weight</th>\n",
       "      <th>viscera_weight</th>\n",
       "      <th>shell_weight</th>\n",
       "      <th>rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4177</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "      <td>4177.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1528</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.523992</td>\n",
       "      <td>0.407881</td>\n",
       "      <td>0.139516</td>\n",
       "      <td>0.828742</td>\n",
       "      <td>0.359367</td>\n",
       "      <td>0.180594</td>\n",
       "      <td>0.238831</td>\n",
       "      <td>9.933684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.120093</td>\n",
       "      <td>0.099240</td>\n",
       "      <td>0.041827</td>\n",
       "      <td>0.490389</td>\n",
       "      <td>0.221963</td>\n",
       "      <td>0.109614</td>\n",
       "      <td>0.139203</td>\n",
       "      <td>3.224169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.441500</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.093500</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.545000</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.799500</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.615000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>1.153000</td>\n",
       "      <td>0.502000</td>\n",
       "      <td>0.253000</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>2.825500</td>\n",
       "      <td>1.488000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>1.005000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sex       length     diameter       height  whole_weight  \\\n",
       "count   4177  4177.000000  4177.000000  4177.000000   4177.000000   \n",
       "unique     3          NaN          NaN          NaN           NaN   \n",
       "top        M          NaN          NaN          NaN           NaN   \n",
       "freq    1528          NaN          NaN          NaN           NaN   \n",
       "mean     NaN     0.523992     0.407881     0.139516      0.828742   \n",
       "std      NaN     0.120093     0.099240     0.041827      0.490389   \n",
       "min      NaN     0.075000     0.055000     0.000000      0.002000   \n",
       "25%      NaN     0.450000     0.350000     0.115000      0.441500   \n",
       "50%      NaN     0.545000     0.425000     0.140000      0.799500   \n",
       "75%      NaN     0.615000     0.480000     0.165000      1.153000   \n",
       "max      NaN     0.815000     0.650000     1.130000      2.825500   \n",
       "\n",
       "        shucked_weight  viscera_weight  shell_weight        rings  \n",
       "count      4177.000000     4177.000000   4177.000000  4177.000000  \n",
       "unique             NaN             NaN           NaN          NaN  \n",
       "top                NaN             NaN           NaN          NaN  \n",
       "freq               NaN             NaN           NaN          NaN  \n",
       "mean          0.359367        0.180594      0.238831     9.933684  \n",
       "std           0.221963        0.109614      0.139203     3.224169  \n",
       "min           0.001000        0.000500      0.001500     1.000000  \n",
       "25%           0.186000        0.093500      0.130000     8.000000  \n",
       "50%           0.336000        0.171000      0.234000     9.000000  \n",
       "75%           0.502000        0.253000      0.329000    11.000000  \n",
       "max           1.488000        0.760000      1.005000    29.000000  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_data = pd.read_csv(\"../ML example/abalone.data\", \n",
    "                            names=[\"sex\", \"length\", \"diameter\", \"height\", \"whole_weight\",\n",
    "                                    \"shucked_weight\", \"viscera_weight\", \"shell_weight\", \n",
    "                                    \"rings\"\n",
    "                            ])\n",
    "abalone_data.describe(include=\"all\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5906717d",
   "metadata": {},
   "source": [
    "2. Try out indexing and selecting data columns by name rater than by column number. The pandas documentation on indexing has further details on how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "61d36f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>rings</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>15</td>\n",
       "      <td>0.455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>7</td>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>9</td>\n",
       "      <td>0.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>10</td>\n",
       "      <td>0.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>7</td>\n",
       "      <td>0.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>F</td>\n",
       "      <td>11</td>\n",
       "      <td>0.565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>M</td>\n",
       "      <td>10</td>\n",
       "      <td>0.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>M</td>\n",
       "      <td>9</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>F</td>\n",
       "      <td>10</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>M</td>\n",
       "      <td>12</td>\n",
       "      <td>0.710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4177 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sex  rings  length\n",
       "0      M     15   0.455\n",
       "1      M      7   0.350\n",
       "2      F      9   0.530\n",
       "3      M     10   0.440\n",
       "4      I      7   0.330\n",
       "...   ..    ...     ...\n",
       "4172   F     11   0.565\n",
       "4173   M     10   0.590\n",
       "4174   M      9   0.600\n",
       "4175   F     10   0.625\n",
       "4176   M     12   0.710\n",
       "\n",
       "[4177 rows x 3 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_data[[\"sex\", \"rings\", \"length\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb9f44e1",
   "metadata": {},
   "source": [
    "## 2.3 Linear Algebra\n",
    "\n",
    "### 2.3.1 Scalars\n",
    "\n",
    "We denote scalars by ordinary lower-cased letters (e.g., x, y, and z) and the space of all continuous *real-valued* scalars by $\\mathbb{R}$.\n",
    "\n",
    "Scalars are implemented as tensors that contain only one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8237f55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "x + y, x * y, x / y, x ** y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "915c7a96",
   "metadata": {},
   "source": [
    "### 2.3.2 Vectors\n",
    "\n",
    "We denote vectors by bold lowercase letters, (e.g, **x**, **y** and **z**).\n",
    "\n",
    "Vectors are implemented as $1^\\text{st}$-order tensors. In general, such tensors can have arbitrary lengths, subject to memory limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "548ede46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "12af9dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0-based indexing\n",
    "x[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47d88040",
   "metadata": {},
   "source": [
    "To indicate that a vector contains $n$ elements, we write $x\\in \\mathbb{R}^n$. Formally, we call $n$ the dimensionality of the vector. In code, this corresponds to the tensor's length, accessible via Python's built-in `len()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0ac55c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e08d86c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also can access shape attribute\n",
    "x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71c8e699",
   "metadata": {},
   "source": [
    "### 2.3.3 Matrices\n",
    "\n",
    "We denote matrices by bold capital letters (e.g., **X**, **Y**, and **Z**), and represent them in code by tensors with two axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "275db5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6).reshape(3,2)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "882d3902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 4],\n",
       "        [1, 3, 5]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flip the axes, A's transpose\n",
    "A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "68451121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a symmetric matrix\n",
    "A = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "A == A.T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "340cb9ad",
   "metadata": {},
   "source": [
    "### 2.3.4 Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "41b8a2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# higher order of tensors\n",
    "torch.arange(24).reshape(2, 3, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07149fea",
   "metadata": {},
   "source": [
    "### 2.3.5 Basic Properties of Tensor Arithmetic\n",
    "\n",
    "Elementwise operations produce outputs that have the same shape as their operands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dc99510f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([[ 0.,  2.,  4.],\n",
       "         [ 6.,  8., 10.]]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "B = A.clone()\n",
    "A, A + B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75271d88",
   "metadata": {},
   "source": [
    "The elementwise product of two matrices is called their ***Hadamard product (denoted $\\odot$)***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "af265e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  4.],\n",
       "        [ 9., 16., 25.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44f85023",
   "metadata": {},
   "source": [
    "Adding or multiplying a scalar and a tensor produces a result with the same shape as the original tensor. Here, each element of the tensor is added to (or multiplied by) the scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "97f1966f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da9998c3",
   "metadata": {},
   "source": [
    "### 2.3.6 Reduction\n",
    "\n",
    "Sum jof the elements in a vector $x$ of length $n$, $\\sum_{i=1}^n x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9df4bab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor(3.))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3, dtype=torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5df222aa",
   "metadata": {},
   "source": [
    "To express sums over the elements of tensors of arbitrary shape, we simply sum over all of its axes. The sum of the elements of an $m\\times n$ matrix $A$ could be written $\\sum_{i=1}^m\\sum_{i=1}^n a_{ij}$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a500ed19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), tensor(15.))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, A.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22911499",
   "metadata": {},
   "source": [
    "Specifying axis = 1 will reduce the column dimension (axis 1) by summing up elements of all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4cb79d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " torch.Size([2, 3]),\n",
       " tensor([ 3., 12.]),\n",
       " torch.Size([2]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, A.shape, A.sum(axis=1), A.sum(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ed550326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce along both rows and columns via sum is equivalent to sum all\n",
    "A.sum(axis=[0, 1]) == A.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7dd6a26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.5000), tensor(2.5000))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the mean\n",
    "A.mean(), A.sum() / A.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "287d542f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.5000, 2.5000, 3.5000]), tensor([1.5000, 2.5000, 3.5000]))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The function for calculating the mean can also reduce a tensor along axes\n",
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2341dd6",
   "metadata": {},
   "source": [
    "### 2.3.7 Non-Reduction Sum\n",
    "\n",
    "Sometimes it can be useful to keep the number of axes unchanged when invoking the function for calculating the sum or mean. This matters when we want to use the broadcast mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "20be6f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.],\n",
       "         [12.]]),\n",
       " torch.Size([2, 1]))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdim=True)\n",
    "sum_A, sum_A.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65590cb4",
   "metadata": {},
   "source": [
    "For instance, since sum_A keeps its two axes after summing each row, we can divide A by sum_A with broadcasting to create a matrix where each row sums up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f5e4b4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.3333, 0.6667],\n",
       "        [0.2500, 0.3333, 0.4167]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / sum_A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98ea379e",
   "metadata": {},
   "source": [
    "If we want to calculate the cumulative sum of elements of A along some axis, say axis=0 (row by row), we can call the cumsum function. By design, this function does not reduce the input tensor along any axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "92ca79b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 5., 7.]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.cumsum(axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1067eb41",
   "metadata": {},
   "source": [
    "### 2.3.8 Dot Products\n",
    "\n",
    "Given two vectors $\\bold{x}, \\bold{y} \\in \\mathbb{R}^d$, their dot product $\\bold{x}^T\\bold{y}$ or $<\\bold{x}, \\bold{y}>$ is a sum over the products of the elements at the same position: $\\bold{x}^T\\bold{y} = \\sum_{i=1}^d x_iy_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c7d04c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(3, dtype = torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fcdb7986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Equivalently, calculate the dot product of two vectors\n",
    "torch.sum(x * y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e131999a",
   "metadata": {},
   "source": [
    "### 2.3.9 Matrix-Vector Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0adfe9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([3]), tensor([ 5., 14.]), tensor([ 5., 14.]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix vector product. @ -> matrix-matrix products\n",
    "A.shape, x.shape, torch.mv(A, x), A@x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0aefaa",
   "metadata": {},
   "source": [
    "### 2.3.10 Matrix-Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5ab9db4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.,  3.,  3.,  3.],\n",
       "         [12., 12., 12., 12.]]),\n",
       " tensor([[ 3.,  3.,  3.,  3.],\n",
       "         [12., 12., 12., 12.]]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(3, 4)\n",
    "torch.mm(A, B), A@B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "132e6d81",
   "metadata": {},
   "source": [
    "### 2.3.11 Norms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f858ffa8",
   "metadata": {},
   "source": [
    "1. $\\ell_2$ norm: Euclidean norm\n",
    "\n",
    "$$\\|x\\|_2=\\sqrt{\\sum_{i=1}^nx_i^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "012bb68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cd86420",
   "metadata": {},
   "source": [
    "2. $\\ell_1$ norm: Manhattan distance\n",
    "\n",
    "$$\\|x\\|_1=\\sum_{i=1}^n|x_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d0f51b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(u).sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0396dd9",
   "metadata": {},
   "source": [
    "3. Frobenius norm, defined as the square root of the sum of the squares of a matrix's elements:\n",
    "\n",
    "$$||\\bold{X}||_F=\\sqrt{\\sum_{i=1}^m\\sum_{j=1}^nx_{ij}^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "230c3897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.ones((4, 9)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e648df53",
   "metadata": {},
   "source": [
    "## 2.5 Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cc9caedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6650c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better create x = torch.arange(4.0, requires_grad=True)\n",
    "x.requires_grad_(True)\n",
    "x.grad # The default value is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0f200fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate our function of x and assign the result to y\n",
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "eeccd930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the gradient of y with respect to x by calling its\n",
    "# backward method\n",
    "\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "37f2ff07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7291ecc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() # Reset the gradient\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "aacc9298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# backward for non-scalar variables\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y.backward(gradient=torch.ones(len(y))) # Faster: y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "22248a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detaching Computation\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bf00a736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "41445af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradients and Python Control Flow\n",
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = B\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
